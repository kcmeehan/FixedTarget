**************************************************************************************************************
This code is used for:
				-> Running UrQMD simulations in parallel
				-> Converting UrQMD output to STARSIM files for 
					simulation in the STAR detector
				-> Running the STARSIM output through the BFC for reconstruction
Note the first steps of this process were run on UC Davis computers, although this is not required. The rest
of the process was completed on RCF.
**************************************************************************************************************

Author: Christopher E. Flores (UC Davis) with some updates by Kathryn Meehan
Email:  chrflores@ucdavis.edu, kcmeehan@ucdavis.edu

**************************************************************************************************************

STEP 0: GET THE CODE
     explanation:  Clone the sturqmd repository to your working directory.

     usage:        You can obtain all of the code for this repository from the public
     		   repositories of the author. This repository includes a sub-repository
		   called urqmdReader which will also be obtained. After cloning the main 
			repository, navigate to the backgroundforpionyield/ directory and do the following:
			  $ git submodule init
			  $ git submodule update

STEP 1: COMPILE UrQMD	
     explanation:  The repository comes with UrQMD 3.4. These instructions should be identical
     		   to those on the UrQMD website. Note that urqmd requires a fortran compiler.
		   Most typical installation of gcc include a fortran compiler by default. However,
		   it is frequently called gfortran-<version> where <version> is the version number.
		   URQMD looks for a binary simply called gfortran. Therefore, it is somtimes necessary
		   to create a symbolic link called gfortran to link to gfortran-<version>. See the linux
		   utility ln (with option s) for how to do this.

     usage:	   Change directory to the urqmd-3.4 (or urqmd-3.4_SL for scientific linux) directory.
     		   Compile the code by doing:
		   	   $ make
			 Do not worry if you get the following message:
			 		 make: *** No rule to make target `inputfile.example', needed by `inputfile'.  Stop.
		   Next do a small run of UrQMD to make the tables
		       	   $ /bin/bash runqmd.bash
	   	   Then change directory back to the main directory
		   	   $ cd ..


STEP 2: COMPILE THE OTHER CODE
     explanation:  The repository contains a minimal amount of code that needs to be compiled.

     usage:	   To compile the remaining code do the following in the parent directory:
     		      $ make

     output:       This should make a bin/ directory if it does not already exist and place the
     		   compiled binaries in bin/.

STEP 3: GENERATE THE URQMD INPUT FILES
     explanation:  Since UrQMD will be run in parallel we need to generate an input file for each 
     		   process to set a different random seed. NOTE: The script is currently set up
		   to supress output to all but the .f13 files. Thus all the .f files except .f13
		   are deleted. If you wish to keep all the files then be sure to change the rm
		   line in runURQMD.sh!

     usage:	   To generate the input files simply execute ./generateURQMDInputFiles.sh in the 
		 scripts directory. The configuration should already be correctly set, but the user 
		 must set the output directory. It is recommended to run only 10 events per job to avoid
		 the job taking too long and being evicted from the queue.
		 **NOTE: The total number of URQMD events that will be simulated is then the product of 
		 eventsPerJob*nJobs. For my analysis I ran 10,000 jobs each with 10 events for total
		 statistics of 100,000 events.

STEP 4: RUN URQMD
     explanation:  Now we run UrQMD using the multiple input files that we generated from the previous step.

     usage:	   UrQMD can be run in two ways: Single instance or in Parallel. The script urqmdSubmitter.sh,
		 found in the scripts directory, submits multiple parallel urqmd jobs by executing the runURQMD.sh 
		 script multiple times. The user must set the output file directory as well as the path to the 
		 inputfiles generated by the generateURQMDInputFiles.sh script. Otherwise the script is already
		 preconfigured for use. Note that the maxJobs option is set to 6 because the UC Davis computers used
		 in this analysis had 6 cores. Once these options have been set, to run the code simply execute the 
		 following in the scripts directory:

		 $ ./urqmdSubmitter.sh 
		   		     
     output: While the script is running you should see it report how many parallel jobs are currently
       running and when it submits a new jobs. Also note that if you look in the output directory
		   while the jobs are running you will see output files .fN where n is any number. If you have
		   suppressed output to some files then those files will be empty and will be removed when 
		   the process finishes. Finally, note that the stdout and stderr streams are redirected to /dev/null

STEP 5: CONVERT URQMD OUTPUT TO ROOT TREES
     explanation: Since UrQMD outputs a specially formated text file we would like to convert the 
		 information in it to a ROOT tree for easier processing and future analysis. In this step we extract 
		 all of the information from the .f13 output files of UrQMD into ROOT trees.

		 NOTE: THIS WILL ONLY WORK WITH THE FORMAT OF THE .f13 FILES!!!

     usage:   This script needs only three arguments:
     		      inputDir	    - Path to the directory containing all of the .f13 files
							outRootDir    - Path to the directory where the resulting ROOT files should be written
							reflect       - This value allows the longitudinal kinematic values of the track to be
		                        reflected in Z. For example, if reflection is turned on pz->-pz. Use 
														reflect=0 for no reflection, use reflect=1 for reflection. The reflection
														option is useful especially for fixed target events that have been 
														simulated in the lab frame. Keep in mind that UrQMD defines the direction
			                      of the projectile to be along the positive z axis.

		      NOTE: The vertex location of all events from the UrQMD simulation is are set to (0,0,0).

					NOTE: The user only needs to set the input and output directories. Everything else is already
					configured for the fxt setup. To execute the code, in the scripts directory type:

		      $ ./runURQMDtoROOT.sh 

STEP 6: GENERATE TX FILES FOR INPUT INTO STARSIM
       explanation:  The detector response simulation for the STAR detector is called STARSIM and is
                   built on GEANT3. STARSIM requires a specially formated text file specifying the
		   properties of each event, each vertex within each event, and each track belonging
		   to each vertex. These files have the ".tx" extension and are made in this step.

		   NOTE: Since the UrQMD events need vertex location information for defining where
		         in the detector they occur, the location information is set in this step.
			 The vertex location is defined by sampling from a z-vertex and xy-vertex
			 histogram obtained from data. Note the name of the TH1D and TH2D vertex
			 histograms should obey the following naming convention:
			     <vertexHistoBaseName>_zVertex and <vertexHistoBaseName>_xyVertex
			 These histograms are generated by the RunVertexPlotsForStarsim.sh script in the analysis repo.

       usage:        This script requires a total of six arguments:
     	 
			 rootDir                - Path to the directory containing all of the root files from the previous step
		   outStarsimDir          - Path to the directory where the resulting .tx files should be saved
		   vertexHistoFile        - Path to and name of the root file containing the vertex histograms
			 
			 The above arguments need to be set by the user. The below arguments should already be configured 
			 correctly. The vertexHistoBaseName might need to be altered if the naming convention in the 
			 RunVertexPlotsForStarsim code was altered.

		   vertexHistoBaseName    - The basename of the vertex histograms (SEE ABOVE NOTE)
		   maxEventsPerOutputFile - Running STARSIM can take a substantial amount of time for high multiplicity
		   			    events. This option allows for only a certain number of events to be written
					    to each output file so that multiple instances of STARSIM can be run in
					    parallel.
		   nDesiredEventsPerJob   - Total number of events that should be written to tx files from each job. Not
		   			    all events in the ROOT file will be transfered to the tx files. For example,
					    events large impact parameters that have no produced particles will be skipped.
					    Use this option to certify that an overal number of events will be present
					    in the .tx files.

		   NOTE: If nDesiredEventsPerJob is larger than the number of good events in the root tree, the tree will
		         be looped over again until nDesiredEventsPerJob is reached. In this case the same event will be
			 used multiple times in the output .tx files, but will be assigned a new vertex location.

       To execute the code, in the scripts directory type:
		   $ ./runMakeStarsimTxFiles.sh 


   ***********************************************************************************************
   ****   Since STARSIM and the BFC require the STAR Libraries and Databases,                 ****
   ****   the following code needs to be run at RCF/PDSF. Pull the repo down there and copy   ****
   ****   all the .tx files you want to process over to RCF before moving on to the           ****
   ****   next step.                                                                          ****
   ***********************************************************************************************

STEP 7: RUN STARSIM
     code:	   STARSIM/StarSim.kumac STARSIM/runSTARSIM.xml 
     
     explanation:  STARSIM can be run by using the runSTARSIM.xml script. This script submits scheduler jobs 
     		           and the scheduler runs STARSIM with the STARSIM/StarSim.kumac file.

     usage:	   Before running this script you need to create a file containing the list of .tx files you
     		   want to process in STARSIM. You can do this by going to the directory containing all of the .tx 
					 files and doing something like:
		       $ ls $PWD/*.tx > myFileList.list

         Note: To make the jobs more efficient and to prevent eviction from the queue in later steps, it
			is recommended to keep the maxFilesPerProcess job tag set to 1 in the xml script. Also, for 
			running over larger statistics, e.g. 100,000 events, it can be easier to use divide the jobs into
			multiple filelists and submit each file list with its own xml on its own terminal window so they can 
			all be submitted at once. For 100,000 events with 10 events/job, this results in 10,000 jobs. To 
			submit them I created 5 filelists each with 2,000 jobs.
	   	   
		   The scriptrunSTARSIM.sh requires six arguments from the user:
		       outDir        - The directory you want to store the output files from STARSIM (.fzd files)
		       logDir        - The directory where the std out and err files should be sent
		       schedDir      - The directory where the scheduler should send all of its auxiliary files
		       inputFileList - The full path and name of the file with the list of input files you created

				 Within the command block of the xml script the user needs to make sure the path to the 
				 StarSim.kumac is set correctly. The geometry and other options should already be configured 
				 correctly. Execute the script the usual way with star-submit runSTARSIM.xml

STEP 8: RUN BFC
     code:	   BFC/bfc.config BFC/runBFC.xml
     
     explanation:  The STAR reconstruction chain can be run using the runBFC.xml script. This script 
		 submits scheduler jobs and the scheduler runs the BFC with the configurations options defined 
		 below.
 
     IMPORTANT:   The BFC is the slowest process in the chain. The length of time it takes to run is 
	  proportional to the total number of tracks in all of the events in the file that it is processing. 
  	  Thus to avoid having jobs evicted for exceeding the queue time limit, it is recommended to set the
	  maxFilesPerProcess job tag to 1. It is also worth considering where you are running the job. For 
	  example as of Jan 2016 the default queue at RCF will guarantee 5 hours of run time, while the 
	  default queue at PDSF will permit up to 24 hours. The jobs that produced the background corrections
	  used in this analysis were run on RCF.

     NOTE:  Since the BFC takes so long to run and its possible to run on multiple files in a single job,
	  the resulting minimc.root files are moved from the scheduler's scratch directory to the output directory
	  as soon as they finish. Thus you should see minimc.root file show up in the output directory while
	  the job they were created in is still running. This also means that if everything ran perfectly
	  and all the files in a job completed, there will be nothing for the scheduler to copy over and you
	  will a "cp no match" in the error file at the end of your job.

     usage: Before running this script you need to create a file containing the list of the BFC files (.fzd) 
	  that you want to process. You can do this as before. 

	  The script requires eight arguments from the user:
		 outDir        - The directory you want to store the output files from the BFC (root files)
		 This should also be set in the mv command near the end of the command block.
		 logDir        - The directory where the std out and err files should be sent
		 schedDir      - The directory where the scheduler should send all of its files
		 inputFileList - The full path and name of the file with the list of input files you created

	  These last two arguments should already be set to working values:
		 first	     - The event in the fzd file you want to start the bfc process on [1,n]
		 last	     - The event in the fzd file you want to end the bfc process on [1,n]
		 				NOTE: If first=last then the first 10000 events will be processed.
		 					If first!=last then the events in the range [first,last] will
		 		be procesed and the output file will have a suffix denoting
		 		the range as well.

	  The star library and chain options should already be set to the correct values. 
	  NOTE: To ensure the vertices are reconstructed using the correct settings for fixed-target it 
	  is critical to use an appropriate database timestamp in the chain option (sdt20150520.155012).  
	  The hour and minute values of the timestamp are necessary in addition to the date, since the 
	  fxt run was only a few hours long and most of the day was spent running p + Au collisions 
	  which have different database settings.

	  Execute the script the usual way with star-submit runBFC.xml

	  Congratulations! You have successfully produced minimc.root files from background simulations! To 
	  process these files to extract the information needed to generate the background curves for futher 
	  analysis, use the RunProcessBackgroundSimulations.xml script in the minimcreader repository.
