This repository contains the code used for the main steps of the Pion dNdy analysis for the 
2015 4.5 GeV Au + Au FXT dataset.

Contact: Kathryn Meehan, kcmeehan@ucdavis.edu

*************************************************************************************************************
STEP 0: Getting started

Clone the repository and in the davisdstanalysis/ directory instantiate the submodules by typing:

git submodule init
git submodule update

Then compile the code by typing:

make

-------------------------------------------------------------------------------------------------------------
STEP 1: Binning the data and obtaining the zTPC distributions

To read in the data set and bin the data into mT-m0 and rapidity bins, run the "skimmer binner" script.
Navigate to the scripts directory and execute:

./RunSkimmerAndBinner.bash

The input and output directories in the script might need to be changed by the user, but otherwise the
rest of the script should already be properly configured. It is possible the STAR library might need to
be changed as well. If SL16a does not work, SL16d should work as an alternative. Other than
inconsequential plot labeling, the main use of the STAR library is in generating the Bichsel curves
(see the ParticleInfo submodule) used during the particle identification step of the analysis (step 2).

-------------------------------------------------------------------------------------------------------------
STEP 2: Fitting the zTPC distributions and extracting the raw spectra

To perform particle identification by fitting the zTPC distributions for each phase space bin, run the
"fitZTPC" script:

./RunFitZTPCPions.sh

The user will need to set the input and output directories, as well as a directory to store the fit
images if desired. Otherwise the rest of the script should already be properly configured. By running
this script the raw spectra for each rapidity bin should be obtained.

-------------------------------------------------------------------------------------------------------------
STEP 3: Generating the background correction curves

Use the BackgroundSimulationsForPiAndProtonYield and the BackgroundMinimcReaderForPiAndProtonYield 
repositories to generate a root file with 3D histograms of the background pions from weak decays and
secondary interactions.

The STARSIM simulation code of the BackgroundSimulationsForPiAndProtonYield repository needs vertex
distribution histograms as input. The script that generates those distributions can be found in this
current repository. To generate these histograms execute:

./RunVertexPlotsForStarsim.sh

Again, the only edits the user should need to make are changing the input and output file paths.
After obtaining the output root file from the BackgroundMinimcReaderForPiAndProtonYield repository, the
background curves can be generated using the RunFeedDownBackgroundAnalysis.C macro of this current
repository. This code can be executed by navigating to the macros/ directory, opening root and typing:

.x RunFeedDownBackgroundAnalysis.C("pathtoinputfile", "pathtooutputfile")

Note only the paths to the input and output file need to be provided by the user.

-------------------------------------------------------------------------------------------------------------
STEP 4: Generating the efficiency X acceptance curves

The minimc files produced by the embedding team do not have a built-in way to determine the centrality
for events in the fixed-target configuration (in this configuration we do not use the standard refmult
variable used by collider analysis for centrality selection since the acceptance is very different in FXT).
To determine the centrality of the events in the minimc files, I crosschecked the eventID and runID of each
event with a "centrality database" table. This is a table of eventIDs, runIDs, and centrality bins for each
event in the dataset that I generated by running the "RunCreateCentralityDatabaseCSV" script over the whole
dataset. This creates a CSV table with the required information. This table must be created before running
the code that reads the minimc files created by the embedding team. This can be done by navigating to the
scripts directory and executing:

./RunCreateCentralityDatabaseCSV.sh

The user only needs to set the path to the data and the path to the output CSV file. This script was
designed to run using the 6 cores of the UC Davis computers in parallel. Thus it produces 6 output csv
files which should be combined into one single CSV file (this can be done simply by piping the output
from cat). This outputfile is then used with the EmbeddingReader repository to process the embedding
minimc files. Please see the README of the EmbeddingReader for details.

After running the code in the EmbeddingReader repository, one should obtain an output root file with
efficiency histograms. This is the input needed for the next step- generating the efficiency X acceptance 
correction curves. To run this code, navigate to the macros directory, open root, and execute:

.x RunMakeEmbeddingCorrectionCurves.C("inputfileWithEfficiencyHistograms","outputfileWithEfficiencyCurves",0,-1)

The user just needs to pass the input file name, and a name for the output file and two numbers indicating
the particle species and the charge. The first number indicates the particle species:
0 -- pions
1 -- kaons
2 -- protons
The second number should be the charge, either -1 or +1. In the case of the pi minus mesons, it should be 
-1 of course. The output file will contain fitted TGraphs of the efficiency X acceptance correction.

-------------------------------------------------------------------------------------------------------------
STEP 5: Obtaining the corrected spectra

Once all the correction curves have been generated, they should be combined into a single root file with
hadd. The corrected spectra can then be generated by navigating to the macros directory, opening root,
and executing the RunCorrectSpectra macro:

.x RunCorrectSpectra.C("pathToRootFileWithRawSpectra","pathToRootFileWithCorrectionCurves",0,-1)

The user just needs to pass the path to the root file with the raw spectra, the path to the root file 
with all the correction cuves, the pid (0 for pions, see the previous step), and the charge of the 
particle of interest, in this case -1. The corrected spectra will then be added in a separate root 
TDirectory within the original raw spectra file.

-------------------------------------------------------------------------------------------------------------
STEP 6: Fitting the spectra and extracting the dNdy

To then fit the spectra and extract the dNdy, one must navigate to the macros directory, open root, and 
execute:

.x RunFitSpectra.C("pathToRootFileWithCorrectedSpectra","outputfilename",0,-1)

The user just needs to pass the path to the root file with the corrected spectra, a path and name for the outputfile, the pid (0 for pions) and the charge of the particle of interest, in this case -1. A new file with fits to the corrected spectra, fits to the temperature (inverse slope parameter) for each rapidity bin, and the extracted dN/dy will be created. This output file is all that is needed to then run a drawing macro to produce the final plots in the paper. 










